# PaperReview
aií˜ì´í¼ ë¦¬ë·°ì´ë©° ë‹¤ì–‘í•œ ëª©ì ì— ë”°ë¼ ììœ ë¡­ê²Œ ê¸°ë¡í•˜ëŠ” ì°½ê³ 


## Member
- https://github.com/jw9603
- [https://github.com/rrr-jh](https://github.com/eloqlo)

## ê¸°ë¡ 


### 2023.08.13
1. Fast R-CNN (ICCV 2015)
   - (https://arxiv.org/abs/1506.01497)
   - ë°œí‘œì : ì´ì¬í˜•
   - ë°œí‘œìë£Œ : https://github.com/Paper-is-all-you-need/PaperReview/blob/main/Fast%20R-CNN.pdf
2. Dynamic Heterogeneous-Graph Reasoning with Language Models and
Knowledge Representation Learning for Commonsense Question
Answering (ACL 2023)
    - (https://aclanthology.org/2023.acl-long.785.pdf)
    - ë°œí‘œì : ì •ì§€ì›
    - ë°œí‘œìë£Œ : TO be continued..




### 2023.11.15
1. Loss Functions for Regression
   - ë°œí‘œì : ì´ì¬í˜•
   - ë°œí‘œìë£Œ : Loss Functions For Regression.pdf(https://github.com/Paper-is-all-you-need/PaperReview/blob/main/Loss%20Functions%20For%20Regression.pdf)


### 2023.11.17
1. Fits : Fine-grained Two-stage Training for knowledge-aware Question Answering (AAAI 2023)
   - https://arxiv.org/abs/2302.11799
   - ë°œí‘œì : ì •ì§€ì›
   - ë°œí‘œëŠ” ë…¼ë¬¸ê³¼ ë³´ì¶©ìë£Œë¥¼ ì´ìš©í•´ì„œ ë°œí‘œí•¨
   - ë°œí‘œ ë³´ì¶©ìë£Œ : https://github.com/Paper-is-all-you-need/PaperReview/tree/main/20231117

### 2024.02.12

1. To briefly summarize several papers to get ideas, the list of papers is as follows:
   
     1. Talk like a graph : Encoding Graphs for Large Language models (ICLR 2024)
       - https://arxiv.org/pdf/2310.04560.pdf
     2. Unifying Large Language Models an Knowledge Graphs : A RoadMap (IEEE Transactions on Knowledge and Data Engineering 2023)
       - https://arxiv.org/pdf/2306.08302.pdf
     3. Reasoning on Graphs : Faithful and Interpretable Large Language Model Reasoning (ICLR 2024)
       - https://arxiv.org/pdf/2310.01061.pdf
     4. Boosting language models reasoning with chain-of-knowledge prompting (ICLR 2024)
       - https://arxiv.org/pdf/2306.06427.pdf
     5. KNOWLEDGE SOLVER: TEACHING LLMS TO SEARCH FOR DOMAIN KNOWLEDGE FROM KNOWLEDGE GRAPHS (arXiv preprint 2023)
       - https://arxiv.org/pdf/2309.03118.pdf
     6. â€œMindmap: Knowledge graph prompting sparks graph of thoughts in large language modelsâ€ (arXiv preprint 2023)
       - https://arxiv.org/pdf/2308.09729.pdf
   - ë°œí‘œì : ì •ì§€ì›
   - ë°œí‘œëŠ” README íŒŒì¼, ë³´ì¶©ìë£Œë¥¼ í†µí•´ì„œ ì§„í–‰
   - ë°œí‘œ ë³´ì¶©ìë£Œ : https://github.com/Paper-is-all-you-need/PaperReview/tree/main/20240212

### 2024.02.16
1. To briefly summarize several papers to get ideas, the list of papers is as follows:

     1. Selection-Inference : Exploiting Large Language Models for Interpretable Logical Reasoning (ICLR 2023)
       - [https://arxiv.org/pdf/2310.04560.pdf](https://arxiv.org/pdf/2205.09712.pdf)
     2. Post Hoc Explanations of Language Models Can Improve Language Models (NeurIPS 2023)
       - [https://arxiv.org/pdf/2306.08302.pdf](https://arxiv.org/pdf/2305.11426.pdf)
     3. Verify-and-Edit : A Knowledge-Enhanced Chain-of-Thought Framework (ACL 2023)
       - [https://arxiv.org/pdf/2310.01061.pdf](https://arxiv.org/pdf/2305.03268.pdf)
     4. Knowledge-Driven CoT : Exploring Faithful Reasoning in LLMs for Knowledge-intensive Question Answering (arXiv preprint 2023)
       - [https://arxiv.org/pdf/2306.06427.pdf](https://arxiv.org/pdf/2308.13259.pdf)
     5. Re-Reading Improves Reasoning in Language Models
       - [https://arxiv.org/pdf/2309.03118.pdf](https://arxiv.org/pdf/2309.06275.pdf) (ICLR 2024)
    
   - ë°œí‘œì : ì •ì§€ì›
   - ë°œí‘œëŠ” README íŒŒì¼
   - ë°œí‘œ ë³´ì¶©ìë£Œ : https://github.com/Paper-is-all-you-need/PaperReview/blob/main/20240216/Paper%20summary.md

2. Depth Anything: Unleashing the Power of Large-Scale Unlabeled Data. Foundation Model for Monocular Depth Estimation (2024))
   - https://arxiv.org/abs/2302.11799
   - ë°œí‘œì : ì´ì¬í˜•
   - ë°œí‘œëŠ” ë…¼ë¬¸ê³¼ ë³´ì¶©ìë£Œë¥¼ ì´ìš©í•´ì„œ ë°œí‘œí•¨
   - ë°œí‘œ ë³´ì¶©ìë£Œ : https://github.com/eloqlo/PaperReview/blob/main/20240216/Depth%20Anything.md



## ì¶”ì²œ ë…¼ë¬¸

ì•„ì§ ì½ì–´ë³´ì§€ëŠ” ì•Šì•˜ì§€ë§Œ í¥ë¯¸ë¡œìš´ ë‚´ìš©ì„ ë‹´ì€ ë…¼ë¬¸ë“¤ì„ ì¶”ì²œ

### 1. Intent-based Prompt Calibration: Enhancing prompt optimization with synthetic boundary cases
   ![image](https://github.com/Paper-is-all-you-need/PaperReview/assets/70795645/600961ee-fb36-466e-8e59-e2c364876343)

   Recommender : ì •ì§€ì›

   ğŸŒ Author(s): Elad Levi, et al.

   ğŸ“… Publication Date: Feb 05, 2024

   âœ¨ Key Insights:

   Whatâ€™s New? They introduced a new method for automatic prompt engineering, using a calibration process that iteratively refines the prompt to the user intent. IPC(Intent-based Prompt         Calibration) outperformed OPRO, and PE.

   Behind the New. During the calibration optimization process, the system iteratively: 1. Suggests a few samples of challenging cases for the task. 2. Evaluates the current prompt on the    
   generated dataset. 3. Given the last few prompts, suggests a new prompt.

   So, How can we use this? Enhance and perfect your prompts for real-world use cases without human labor!

